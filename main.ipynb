{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home work 3: What movie to watch tonight?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import os\n",
    "import urllib\n",
    "import requests\n",
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "import math\n",
    "import heapq\n",
    "import numpy as np\n",
    "\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from utils import CleanData\n",
    "from collections import defaultdict\n",
    "from bs4 import BeautifulSoup\n",
    "from utils import CleanData, RemoveDuplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final output of this task is tsv files of whole movies that makes our dataset.\n",
    "\n",
    "For this aim, first of all we download all html pages.\n",
    "For downloading the pages,we use 3 folders that contain html links for reading the movies' links. Because the times of crawling wikipedia pages are too much, we use a random period between 1 to 5 seconds between two crawling. If the wikipedia website blocks our IP, we consider 20 minutes delay to get out of black list and continue downloading.\n",
    "\n",
    "The code taht we bring here downloading movie2 folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I create a beautifulsoup object\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/CriMenghini/ADM/master/2019/Homework_3/data/movies2.html'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "os.chdir(r'C:\\Users\\cecco\\Desktop\\AllMovies')\n",
    "lst = []\n",
    "i = 0\n",
    "for link in soup.find_all('a'):\n",
    "# with 'href' i take all links\n",
    "\n",
    "    x = link.get('href')\n",
    "    lst.append(x)\n",
    "for v in lst:\n",
    "    try:\n",
    "        # i store the response in a file html\n",
    "        \n",
    "        url = v\n",
    "        response = urllib.request.urlopen(url)\n",
    "        c = \"article_\" + str(i)\n",
    "        webContent = response.read()\n",
    "        f = open(c + '.html', 'wb')\n",
    "        f.write(webContent)\n",
    "        f.close\n",
    "        time.sleep(random.randint(1, 5))\n",
    "        \n",
    "        # I repeat the operation if I generate an excpetion timeout I stop 1200 seconds\n",
    "        \n",
    "    except requests.exceptions.Timeout as e:\n",
    "        time.sleep(1200)\n",
    "        url = v\n",
    "        response = urllib.request.urlopen(url)\n",
    "        c = \"article_\" + str(i)\n",
    "        webContent = response.read()\n",
    "        f = open(c + '.html', 'wb')\n",
    "        f.write(webContent)\n",
    "        f.close\n",
    "        time.sleep(random.randint(1, 5))\n",
    "        \n",
    "        # if I generate an exception for some other reason I skip the file\n",
    "        \n",
    "    except Exception as e:\n",
    "        continue\n",
    "    finally:\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we extract title, intro, plot and some data from infobox.\n",
    "\n",
    "The title is the text in the first heading of wikipedia html code.\n",
    "\n",
    "Intro is first paragraph(s) that ends before \"contents\". The class of \"contents\" is \"toc\". So, we find paragraph(s) before this class.\n",
    "\n",
    "Plot has a specific class that called \"plot\". So, we just find the paragraph(s) inside this class.\n",
    "\n",
    "For gathering data from infobox, we find infobox table. separate headings and contents in 2 lists and by searching in heading list, extract what we want.\n",
    "\n",
    "At the end, collect all this data in a dictionary and save it as tsv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for loop to crawl all the html file that we downloaded before\n",
    "\n",
    "for i in range(29992):\n",
    "    with open(r'C:\\Users\\shekoufeh\\movies3\\article_'+str(i)+'.html' ,encoding ='utf8') as f:\n",
    "        html = f.read()\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "    dict={}\n",
    "    \n",
    "#title\n",
    "\n",
    "    title=(soup.select('.firstHeading'))[0].text\n",
    "    dict['title']=title.replace(\"\\n\",\" \").strip() \n",
    "\n",
    "#intro\n",
    "    \n",
    "    for items in soup.select(\".toc\"):\n",
    "        \n",
    "#the paragraphs before class=\"toc\" are in the intro cluster. so we find and merge them as a intro\n",
    "        \n",
    "        intro = [item.text for item in items.find_previous_siblings() if item.name==\"p\"]\n",
    "        \n",
    "#this method find the paragrapghs from end to top of the page. so we reverse them to be in the correct order\n",
    "\n",
    "    intro.reverse()\n",
    "    \n",
    "    intro=\"\".join(intro)\n",
    "    dict['intro']=intro.replace(\"\\n\",\" \").strip()   \n",
    "    \n",
    "#plot\n",
    "       \n",
    "    try:\n",
    "        plot = []\n",
    "\n",
    "# find the node with id of \"Plot\"\n",
    "\n",
    "        mark = soup.find(id=\"Plot\")\n",
    "        \n",
    "# walk through the siblings of the parent (H2) node until we reach the next H2 node\n",
    "\n",
    "        for elt in mark.parent.nextSiblingGenerator():\n",
    "            if elt.name == \"h2\":\n",
    "                break\n",
    "            if hasattr(elt, \"text\"):\n",
    "                plot.append(elt.text)\n",
    "        \n",
    "# convert to text \n",
    "\n",
    "        plot=\"\".join(plot)\n",
    "    except:\n",
    "        plot=''\n",
    "    \n",
    "    finally:\n",
    "        dict['plot']=plot.replace(\"\\n\",\" \").strip()\n",
    "    \n",
    "#extract infobox\n",
    "\n",
    "    table=(soup.select(\".infobox\"))[0]\n",
    "\n",
    "    output_rowc1 = []\n",
    "    output_rowc0 = []\n",
    "#gather each columns of infobox in a separate list \n",
    "\n",
    "    for table_row in table.findAll('tr'):\n",
    "        rowc1 = table_row.findAll('td')\n",
    "        rowc0 = table_row.findAll('th')\n",
    "        \n",
    "        for row in rowc1:\n",
    "            output_rowc1.append(row.text)\n",
    "        for row in rowc0:\n",
    "            output_rowc0.append(row.text)\n",
    "\n",
    "#add information in infobox to dictionary\n",
    "            \n",
    "    #film_name\n",
    "    dict['film_name']=output_rowc0[0]\n",
    "\n",
    "##search information in the lists, if they were available, we add them to a dictionary, otherwise, we add \"NA\" to dictionary  \n",
    "    \n",
    "    #director\n",
    "    a = [output_rowc0.index(i) for i in output_rowc0 if \"direct\" in i.lower()]\n",
    "    if len(a)>0:\n",
    "        dict['director']=output_rowc1[a[0]].replace(\"\\n\",\"\").strip() \n",
    "    else:\n",
    "        dict['director']=\"NA\"\n",
    "    \n",
    "    #producer\n",
    "    a = [output_rowc0.index(i) for i in output_rowc0 if \"produce\" in i.lower()]\n",
    "    if len(a)>0:\n",
    "        dict['producer']=output_rowc1[a[0]].replace(\"\\n\",\"\").strip()  \n",
    "    else:\n",
    "        dict['producer']=\"NA\"\n",
    "    \n",
    "    #writer\n",
    "    a = [output_rowc0.index(i) for i in output_rowc0 if \"writ\" in i.lower()]\n",
    "    if len(a)>0:\n",
    "        dict['written']=output_rowc1[a[0]].replace(\"\\n\",\"\").strip()\n",
    "    else:\n",
    "        dict['written']=\"NA\"\n",
    "    \n",
    "    #starring\n",
    "    a = [output_rowc0.index(i) for i in output_rowc0 if \"star\" in i.lower()]\n",
    "    if len(a)>0:\n",
    "        dict['starring']=output_rowc1[a[0]].replace(\"\\n\",\"\").strip()\n",
    "    else:\n",
    "        dict['starring']=\"NA\"\n",
    "    \n",
    "    #music\n",
    "    a = [output_rowc0.index(i) for i in output_rowc0 if \"music\" in i.lower()]\n",
    "    if len(a)>0:\n",
    "        dict['music']=output_rowc1[a[0]].replace(\"\\n\",\"\").strip()\n",
    "    else:\n",
    "        dict['starring']=\"NA\"\n",
    "    \n",
    "    #release date\n",
    "    a = [output_rowc0.index(i) for i in output_rowc0 if \"release\" in i.lower()]\n",
    "    if len(a)>0:\n",
    "        dict['release date']=output_rowc1[a[0]].replace(\"\\n\",\"\").strip()\n",
    "    else:\n",
    "        dict['release date']=\"NA\"\n",
    "    \n",
    "    #running time\n",
    "    a = [output_rowc0.index(i) for i in output_rowc0 if \"run\" in i.lower()]\n",
    "    if len(a)>0:\n",
    "        dict['runtime']=output_rowc1[a[0]].replace(\"\\n\",\"\").strip()\n",
    "    else:\n",
    "        dict['runtime']=\"NA\"\n",
    "    \n",
    "    #country\n",
    "    a = [output_rowc0.index(i) for i in output_rowc0 if \"country\" in i.lower()]\n",
    "    if len(a)>0:\n",
    "        dict['country']=output_rowc1[a[0]].replace(\"\\n\",\"\").strip()\n",
    "    else:\n",
    "        dict['country']=\"NA\"\n",
    "    \n",
    "    #language\n",
    "    b = [output_rowc0.index(i) for i in output_rowc0 if \"language\" in i.lower()]\n",
    "    if len(a)>0:\n",
    "        dict['language']=output_rowc1[a[0]].replace(\"\\n\",\"\").strip()\n",
    "    else:\n",
    "        dict['language']=\"NA\"\n",
    "    \n",
    "    #budget\n",
    "    a = [output_rowc0.index(i) for i in output_rowc0 if \"budget\" in i.lower()]\n",
    "    if len(a)>0:\n",
    "        dict['budget']=output_rowc1[a[0]].replace(\"\\n\",\"\").strip()\n",
    "    else:\n",
    "        dict['budget']=\"NA\"\n",
    "        \n",
    "#create tsv files\n",
    "        \n",
    "    with open('article_'+str(i)+'tsv','w',encoding='utf8') as f:\n",
    "        fieldnames = ['title','intro','plot','film_name','director', 'producer', 'written', 'starring', 'music', 'release date', 'runtime', 'country', 'language', 'budget']\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames,dialect=\"excel-tab\")\n",
    "        writer.writeheader()\n",
    "        writer.writerow(dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Search Engine "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Conjunctive query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to create two different Search Engines that, given as input a query, return the movies that match the query.\n",
    "\n",
    "For the homework it has been requested to create a vocabulary file that associates to each word a term_id. Here we are going to define the function used to create the first inverted_index, which will associate to term_id word the list of tsv documents which contain it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with this function I create the vocabulary and the first index file\n",
    "\n",
    "def index1():\n",
    "    vocabulary ={}\n",
    "    inverted_idx=defaultdict(list)\n",
    "    cnt = 1\n",
    "    for i in range(29992):\n",
    "        try:\n",
    "            db = pd.read_csv(r'C:\\Users\\cecco\\Desktop\\PagWeb\\article_' + str(i) +'.tsv', sep='\\t', encoding = 'utf8')\n",
    "        except FileNotFoundError as e:\n",
    "            continue\n",
    "        # i take only intro and plot\n",
    "        intro = db['intro']\n",
    "        plot = db['plot']\n",
    "        # i clean data and remove duplicates\n",
    "        for x in intro:\n",
    "            IntroWords = CleanData(x)\n",
    "        for y in plot:\n",
    "            PlotWords = CleanData(y)\n",
    "        AllWords = RemoveDuplicates(IntroWords,PlotWords)\n",
    "        # I add the words with their id to the vocabulary\n",
    "        for token in AllWords:\n",
    "            if token in vocabulary.keys():\n",
    "                continue\n",
    "            else:\n",
    "                vocabulary.update({token:cnt})\n",
    "                cnt += 1\n",
    "        # in the index I add key of the word and articles in which it is found\n",
    "        for w in AllWords:\n",
    "            x = vocabulary[w]\n",
    "            inverted_idx[x].append('article_' + str(i))\n",
    "    os.chdir(r'C:\\Users\\cecco\\Desktop\\PagWeb')\n",
    "    # I save the vocabulary and the inverted index in two file\n",
    "    with open(\"vocabulary.json\", \"w\", encoding = \"utf8\") as myfile:\n",
    "        myfile.write(json.dumps(vocabulary))\n",
    "    with open(\"inverted_index.json\", \"w\", encoding = \"utf8\") as myfile:\n",
    "        myfile.write(json.dumps(inverted_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a query the Search Engine is supposed to return a list of documents. The conjunctive queries (AND) must br return each documents that contain all the words in the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchEngine_1(query):\n",
    "    query = str(query)\n",
    "    query = CleanData(query)\n",
    "    # I load the vocabulary and the inverted index so as not to have to calculate them at the moment\n",
    "    inverted_index = json.loads(open(\"inverted_index.json\").read())\n",
    "    vocabulary = json.loads(open(\"vocabulary.json\").read())\n",
    "    index = []\n",
    "    # I get the word id from the vocabulary\n",
    "    for par in set(query):\n",
    "        if par in vocabulary.keys():\n",
    "            index.append(vocabulary[par])\n",
    "    docs = []\n",
    "    if len(index) != len(set(query)):\n",
    "        # not all words are in the dictionary\n",
    "        return print(\"There are no results that match your query.\")\n",
    "    else:\n",
    "        # I get the set of all the articles they have all the words from\n",
    "        for indices in index:\n",
    "            docs.append(set(inverted_index[str(indices)]))\n",
    "        docs = set.intersection(*docs)\n",
    "        if len(docs) == 0:\n",
    "            # no film has all the words\n",
    "            return print('there are no articles that contain all the words.')\n",
    "        else:\n",
    "            # creo un db con l'insieme dei film e per ognuno stampo title intro plot e url\n",
    "            result = pd.DataFrame()\n",
    "            d_url = json.loads(open(\"d_url.json\").read())\n",
    "            for doc in docs:\n",
    "                z = d_url[doc]\n",
    "                df1 = pd.DataFrame({'link': z}, index=[0])\n",
    "                df = pd.read_csv(doc + \".tsv\", delimiter=\"\\t\")\n",
    "                result2 = pd.concat([df, df1], axis=1)\n",
    "                result = result.append(result2)\n",
    "            result.reset_index(drop=True, inplace=True)\n",
    "            return (result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Conjunctive query & Ranking score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we need to create and inverted_score_index. This index will associate to each word, a list tuple (document, tfidf), with all the documents containing that word, and another file where I store only idf this will be useful to calculate the tfidf of the words in the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index2():\n",
    "    os.chdir(r'C:\\Users\\cecco\\Desktop\\PagWeb')\n",
    "    inverted_index = json.loads(open(\"inverted_index.json\").read())\n",
    "    vocabulary = json.loads(open(\"vocabulary.json\").read())\n",
    "    inverted2_idx = defaultdict(list)\n",
    "    Idf_dict = defaultdict(list)\n",
    "    for i in range(29992):\n",
    "        try:\n",
    "            db = pd.read_csv(r'C:\\Users\\cecco\\Desktop\\PagWeb\\article_' + str(i) + '.tsv', sep='\\t', encoding='utf8')\n",
    "        except FileNotFoundError as e:\n",
    "            continue\n",
    "        intro = db['intro']\n",
    "        plot = db['plot']\n",
    "        for x in intro:\n",
    "            IntroWords = CleanData(x)\n",
    "        for y in plot:\n",
    "            PlotWords = CleanData(y)\n",
    "        Plot_Intro = IntroWords + PlotWords\n",
    "        \n",
    "        # I take the length of intro and plot to calculate the tf\n",
    "        lungh = len(Plot_Intro)\n",
    "        AllWords = RemoveDuplicates(IntroWords, PlotWords)\n",
    "        \n",
    "        # update current vocabulary with the new words found in a document\n",
    "        tfIdf = {}\n",
    "        IDF = {}\n",
    "        for token1 in AllWords:\n",
    "            \n",
    "            # I count the number of times the word appears in intro and plot always useful for calculating the tf\n",
    "            x = Plot_Intro.count(token1)\n",
    "            id_token = vocabulary[token1]\n",
    "            doc_par = inverted_index[str(id_token)]\n",
    "            \n",
    "            # check in how many articles the word useful to calculate idf appears\n",
    "            num_doc_par = len(doc_par)\n",
    "            # calculate the tf\n",
    "            tf = x / lungh\n",
    "            # calculate idf\n",
    "            Idf = 1 + math.log(29992 / num_doc_par)\n",
    "            IDF[token1] = Idf\n",
    "            # calculate tfidf\n",
    "            tfIdf[token1] = tf * Idf\n",
    "\n",
    "        for ww in AllWords:\n",
    "            l = tfIdf[ww]\n",
    "            p = IDF[ww]\n",
    "            xx = vocabulary[ww]\n",
    "            inverted2_idx[xx].append(('article_' + str(i), l))\n",
    "            Idf_dict[xx].append(('article_' + str(i), p))\n",
    "            \n",
    "            # I create a file by copying into it a dictionary whose key is the id of the word and as values ​\n",
    "            # a tuple formed by the document of belonging and its tfidf\n",
    "            \n",
    "    with open(\"inverted2_index.json\", \"w\", encoding=\"utf8\") as myfile:\n",
    "        myfile.write(json.dumps(inverted2_idx))\n",
    "        \n",
    "        # I create a file by copying into it a dictionary whose key is the id of the word and as values ​\n",
    "        # a tuple formed by the document of belonging and its idf this will be useful to calculate the tfidf of the words in the query\n",
    "    \n",
    "    with open(\"Idf_dict.json\", \"w\", encoding=\"utf8\") as myfile:\n",
    "        myfile.write(json.dumps(Idf_dict))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the scored index we can define the functions for the execution of the query. In this section, Cosine Similarity is used to see how much the query is equal to the document found.\n",
    "\n",
    "To calculate the Cosine Similarity between each document and the query we need to be able to compute the dot product between 2 list of floats and the norm of a list of floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchEngine_2(query):\n",
    "    query = CleanData(query)\n",
    "    inverted_index = json.loads(open(\"inverted_index.json\").read())\n",
    "    inverted2_index = json.loads(open(\"inverted2_index.json\").read())\n",
    "    vocabulary = json.loads(open(\"vocabulary.json\").read())\n",
    "    Idf_dict = json.loads(open(\"Idf_dict.json\").read())\n",
    "    index = []\n",
    "    \n",
    "    # I calculate the tf of the word in the query since the query is a set is always given by 1 fraction the length of the set\n",
    "    query_words_tf = 1/len(query)\n",
    "    query = set(query)\n",
    "    \n",
    "    # this part is identical to the search engine 1\n",
    "    for par in query:\n",
    "        if par in vocabulary.keys():\n",
    "            index.append(vocabulary[par])\n",
    "    docs = []\n",
    "    if len(index) != len(query):\n",
    "        return print(\"There are no results that match your query.\")\n",
    "    else:\n",
    "        for indices in index:\n",
    "            docs.append(set(inverted_index[str(indices)]))\n",
    "        docs = set.intersection(*docs)\n",
    "        if len(docs) == 0:\n",
    "            return print('there are no articles that contain all the words.')\n",
    "        else:\n",
    "            lst2 = []\n",
    "           \n",
    "        # for each document\n",
    "            for d in docs:\n",
    "                lst = []\n",
    "                lst1 = []\n",
    "               \n",
    "            # for word in the document i calculate the cosine similarity between words in the document and words in the query\n",
    "                for i in index:\n",
    "                    c = inverted2_index[str(i)]\n",
    "                    for val in c:\n",
    "                        if val[0] == d:\n",
    "                            lst.append(val[1])\n",
    "                    b = Idf_dict[str(i)]\n",
    "                    for val1 in b:\n",
    "                        if val1[0] == d:\n",
    "                            lst1.append(val1[1]*query_words_tf)\n",
    "                cosine  = [d,round(dot(lst, lst1) / (norm(lst) * norm(lst1)),3)]\n",
    "                lst2.append(cosine)\n",
    "               \n",
    "            # I create a maxheap where I keep documents with relative similarity\n",
    "            q = [(x[1], x[0]) for x in lst2]\n",
    "            heapq._heapify_max(q)\n",
    "            k = 5\n",
    "            lst4 = []\n",
    "            for i in range(k):\n",
    "                    try:\n",
    "                        c = heapq._heappop_max(q)\n",
    "                        lst4.append(c)\n",
    "                    except IndexError:\n",
    "                        break\n",
    "            result = pd.DataFrame()\n",
    "           \n",
    "        # I create the db to be returned with the first 5 elements with greater similarity\n",
    "            d_url = json.loads(open(\"d_url.json\").read())\n",
    "            for tupla in lst4:\n",
    "                z = d_url[tupla[1]]\n",
    "                df2 = pd.DataFrame({'similarity' : tupla[0]},index=[0])\n",
    "                df1 = pd.DataFrame({'link': z}, index=[0])\n",
    "                df = pd.read_csv(tupla[1] + \".tsv\", delimiter=\"\\t\")\n",
    "                result1 = df[['title', 'intro', 'plot']]\n",
    "                result2 = pd.concat([result1, df1,df2], axis=1)\n",
    "                result = result.append(result2)\n",
    "            result.reset_index(drop=True, inplace=True)\n",
    "            print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this part it is possible to choose the search engine to use and the query to be introduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have the user choose which dimilarity to use and the query to insert\n",
    "n = input(\"Enter query number: \")\n",
    "if n == str(1):\n",
    "    query = input(\"Please, write a query: \")\n",
    "    df = searchEngine_1(query)\n",
    "    if df is None:\n",
    "        pass\n",
    "    else:\n",
    "        print(df[['title', 'intro', 'plot', 'link']])\n",
    "if n == str(2):\n",
    "    query = input(\"Please, write a query: \")\n",
    "    searchEngine_2(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Define a new score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we should rank the output of part 2.1.\n",
    "For defining a function to rank the list of movies, we can use any variables except into and plot.\n",
    "\n",
    "We decided to ask the user 4 questions for ranking the list of movies and give a weight to each question. 4 variables that asked from user is film_name, director, starring and language. \n",
    "The most important variable is film_name. So, we consider 2 type of weights for variables in the condition of answering this question or not.\n",
    "\n",
    "Another hypothesis is that the default answer for language question is \"English\". So, if the user don't answer to the language question or any questions, we could have a weighted array for calculating scores that leads us to new ranking.\n",
    "\n",
    "How the scoring function works?\n",
    "\n",
    "The function consider a  1*4 array with zero elements for each movie of dataframe. If the entries of user for each question be found in the dataframe, the amount of array change from zero into 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input of the function has to be the result from the step 2.1 that contains all the columns\n",
    "\n",
    "def searchEngine_3(df):  # when we have the film name\n",
    "    w1 = [0.8, 0.1, 0.05, 0.05]\n",
    "    # when we don't have the film name\n",
    "    w2 = [0.0, 0.5, 0.2, 0.3]\n",
    "    # Requesting inputs from user.\n",
    "    Film_name = input(\"If you are looking for special movie enter the film name otherwise enter space:\\n \")\n",
    "    Starring = input(\"If you are looking for actor enter the actor name otherwise enter space:\\n \")\n",
    "    Director = input(\"If you are looking for special movie Director enter the Director name otherwise enter space:\\n \")\n",
    "    language = input(\" what is your preferred language:? \\n \")\n",
    "    if language == '':\n",
    "        language = 'English'\n",
    "    for i in range(len(df)):\n",
    "        s = [0, 0, 0, 0]\n",
    "        try:\n",
    "            if Film_name != '':\n",
    "                if df.loc[i, 'film_name'].lower().find(Film_name.lower()) != -1:\n",
    "                    s[0] = 1\n",
    "        except:\n",
    "            continue\n",
    "        try:\n",
    "            if Starring != '':\n",
    "                if df.loc[i, 'starring'].lower().find(Starring.lower()) != -1:\n",
    "                    s[1] = 1\n",
    "        except:\n",
    "            continue\n",
    "        try:\n",
    "            if Director != '':\n",
    "                if df.loc[i, 'director'].lower().find(Director.lower()) != -1:\n",
    "                    s[2] = 1\n",
    "        except:\n",
    "            continue\n",
    "        try:\n",
    "            if language != '':\n",
    "                if df.loc[i, 'language'].lower().find(language.lower()) != -1:\n",
    "                    s[3] = 1\n",
    "        except:\n",
    "            continue\n",
    "        finally:\n",
    "            if Film_name == '':\n",
    "                DotProduct = np.dot(s, w2)\n",
    "                df.loc[i, 'Score'] = DotProduct\n",
    "            else:\n",
    "                DotProduct = np.dot(s, w1)\n",
    "                df.loc[i, 'Score'] = DotProduct\n",
    "    return (df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the function calculates the score of movies by producting this array to weight array, we have a new ranking by sorting the dataframe based on new scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if the output of search engine_1 is res_searchEngine1 and contains all the informations in infobox\n",
    "\n",
    "    if res_searchEngine1 is None:\n",
    "        pass\n",
    "    else:\n",
    "        df = searchEngine_3(res_searchEngine1)\n",
    "        result = df.sort_values(by = ['Score'],ascending=False)\n",
    "        result.reset_index(drop=True, inplace=True)\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Algorithmic question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function take in input one value the the string of which we must calculate the palindrome. To calculate the palindrome I use the dynamic programming employing time n^2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the length of the maximum palindrome\n",
    "#is how to calculate the length of the maximum \n",
    "#sub-sequence between the input string and its inverse\n",
    "def Palindroma(X): \n",
    "    # calculation Y as the inverse of X\n",
    "    Y = X[::-1]\n",
    "    # calculate the length of the string X to then create a matrix where to save the values\n",
    "    n = len(X) \n",
    "    Matrix = [[None]*(n + 1) for i in range(n + 1)]\n",
    "\n",
    "    for i in range(n + 1): \n",
    "        for j in range(n + 1): \n",
    "            if i == 0 or j == 0 : \n",
    "                Matrix[i][j] = 0\n",
    "            elif X[i-1] == Y[j-1]: \n",
    "                Matrix[i][j] = Matrix[i-1][j-1]+1\n",
    "            else: \n",
    "                Matrix[i][j] = max(Matrix[i-1][j], Matrix[i][j-1])\n",
    "  \n",
    "    # Matrix[n][n] contains the length of Palindroma of X[0..n-1] & Y[0..n-1]\n",
    "    return Matrix[n][n]\n",
    "Palindroma('DATAMININGSAPIENZA')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
